{
    "name": "llama_2m",
    "n_embd": 64,
    "intermediate_size": 256,
    "n_head": 2,
    "n_layer": 2,
    "vocab_size": 1024
}

 